{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f92ca115",
   "metadata": {},
   "source": [
    "# 14 - Pass, containers and for\n",
    "\n",
    "This notebook is a serie of exercises about the concept presented in [14 Pass, containers, for] and made by [Theo van Walsum](mailto:t.vanwalsum@erasmusmc.nl)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f1ace5",
   "metadata": {},
   "source": [
    "- Those exercises are not mandatory but it is strongly advised to do them as programming is skill learnt by doing\n",
    "- Exercise have an associated difficulty level: 1 means that only an understanding of the course is sufficient to complete the exercise, 2 means that some research is needed to complete the exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a677ae",
   "metadata": {},
   "source": [
    "## Exercise 1: Calculating term frequencies in the document\n",
    "In information retrieval, [tf–idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) (short for term frequency–inverse document frequency) is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. Another common problem is that frequently occuring words (e.g. articles, prepositions, and auxiliary verbs) can skew the statistics. They are commonly assembled into a stoplist which is then used to filter the document.\n",
    "\n",
    "In the first part of the exercise we will focus on calculating the term frequencies for words from a single document. Term frequency (TF) is in its simplest form calculated by counting all occurences of a word in a document and then dividing it by a total number of words in the document. \n",
    "\n",
    "Create a function that takes a document filename as an argument and outputs all words from the document together with their term frequencies. Word case should be ignored, e.g. `Word` and `word` should be counted together. The function should return a single data structure. All words from a stoplist should not be counted in the calculations. You can create your own stoplist of at least 20 words or use one of publicly available stoplists. We provide you with a test file (`emma-tf.txt`)  where all punctuation marks have been removed and all words are separated by a space character. Pay close attention to your choice of data structures. Find the most frequent word in the text. Should it have been included in the stoplist you have used?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23135096",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "# Solution 1.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bc848c",
   "metadata": {},
   "source": [
    "## Exercise 2: Markov analysis (level of difficulty: 2)\n",
    "\n",
    "This exercise is inspired by / taken from the 'Think Python' (Allen B. Downey). \n",
    "\n",
    "In a Markov process, transition to the next state is only determined by the current state. Such Markov processes can be used to model e.g. text. In this exercises, you will implement such a text processing modeling system, and use it to generate text.\n",
    "\n",
    "In text processing, the 'current state' can be represented by the previous N words, and the predicted new state is the next word. We can build such a model by reading a text, and then for every set of N words, store each next word (or better: all possible next words, including their frequency). \n",
    "\n",
    "The first exercise is to create a function that builds such a model when reading a text file. The main task is to decide on the appropriate datastructures for building this model. In building the model, you must also take into account that the same model will be used to generate new text from this model.\n",
    "\n",
    "As an input file, the [emma.txt](https://github.com/AllenDowney/ThinkPython2/blob/master/code/emma.txt) has been prepared. All dashes, underscores and quotation marks have been stripped. What remains is text, with upper and lower case characters, and the following punctuation marks: ' , . : ; ! ? . For this exercise, you may consider these punctuation marks as words. For your convenience, the input text has been prepared such that these punctuation marks are separated by spaces before and after, such that all input words are space (space / newline / tab) separated. You do not need to change the case of the characters.\n",
    "\n",
    "In addition, there is a simpler text file, called test.txt in the data_14 folder. You may use that to e.g. print the result of the model building.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a4fe3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 2.1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9820a607",
   "metadata": {},
   "source": [
    "Write code to create random sentences. Think of a good way to initialize the first state in a nice and random manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5570bd41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 2.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983d8c7c",
   "metadata": {},
   "source": [
    "# Exercise 2: Markov model for words (level of difficulty: 2)\n",
    "\n",
    "The same model building approach can be applied to modeling words instead of sentences. Do the same for words. In reading words, only include words of at least key_len + 1 char, and convert all words to lower case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f864b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc22d2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 3.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4650dd",
   "metadata": {
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a70d306b",
   "metadata": {},
   "source": [
    "Given the models build, you may want to get more information. E.g. which state has the highest number of possible next words. And how many different words are in that list. And how many states do have exactly one (deterministic) subsequent word. And what is the histogram of the number of words per state (i.e. visualize in a graph the number of states with exactly one word in the list, and with two words in the list, etc). And how do these histograms vary, e.g. as a function of key_len (what do you expect), of text (or author), or of language. It is left as a further exercise (without solution) to program solutions that may answer such questions. "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_markers": "\"\"\""
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
