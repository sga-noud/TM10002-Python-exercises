{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f92ca115",
   "metadata": {},
   "source": [
    "# 14 - Pass, containers and for\n",
    "\n",
    "This notebook is a serie of exercises about the concept presented in [14 Pass, containers, for] and made by [Theo van Walsum](mailto:t.vanwalsum@erasmusmc.nl)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f1ace5",
   "metadata": {},
   "source": [
    "- Those exercises are not mandatory but it is strongly advised to do them as programming is skill learnt by doing\n",
    "- Exercise have an associated difficulty level: 1 means that only an understanding of the course is sufficient to complete the exercise, 2 means that some research is needed to complete the exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a677ae",
   "metadata": {},
   "source": [
    "## Exercise 1: Calculating term frequencies in the document\n",
    "In information retrieval, [tf–idf](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) (short for term frequency–inverse document frequency) is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus. The tf–idf value increases proportionally to the number of times a word appears in the document and is offset by the number of documents in the corpus that contain the word, which helps to adjust for the fact that some words appear more frequently in general. Another common problem is that frequently occuring words (e.g. articles, prepositions, and auxiliary verbs) can skew the statistics. They are commonly assembled into a stoplist which is then used to filter the document.\n",
    "\n",
    "In the first part of the exercise we will focus on calculating the term frequencies for words from a single document. Term frequency (TF) is in its simplest form calculated by counting all occurences of a word in a document and then dividing it by a total number of words in the document. \n",
    "\n",
    "Create a function that takes a document filename as an argument and outputs all words from the document together with their term frequencies. Word case should be ignored, e.g. `Word` and `word` should be counted together. The function should return a single data structure. All words from a stoplist should not be counted in the calculations. You can create your own stoplist of at least 20 words or use one of publicly available stoplists. We provide you with a test file (`emma-tf.txt`)  where all punctuation marks have been removed and all words are separated by a space character. Pay close attention to your choice of data structures. Find the most frequent word in the text. Should it have been included in the stoplist you have used?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23135096",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most frequent word is 'mr.' with 1153 times \n",
      "\n",
      "[('mr.', 1153), ('emma', 862), ('all', 846), ('could', 837), ('would', 820), ('him', 769), ('been', 760), ('no', 742), ('my', 732), ('mrs.', 700), ('on', 691), ('any', 655), ('do', 635), ('miss', 602), ('me', 583), ('must', 571), ('will', 559), ('which', 556), ('what', 536), ('harriet', 505), ('or', 494), ('such', 489), ('much', 486), ('if', 485), ('said', 484), ('more', 468), ('are', 455), ('one', 453), ('weston', 439), ('every', 435), ('them', 433), ('am', 425), ('than', 416), ('thing', 398), ('knightley', 389), ('elton', 387), ('think', 384), ('well', 381), ('how', 371), ('should', 371), ('your', 367), ('when', 364), ('little', 360), ('being', 358), ('never', 358), ('did', 353), ('only', 341), ('know', 337), ('might', 326), ('good', 313), ('woodhouse', 312), ('say', 311), ('their', 306), ('own', 303), ('jane', 300), ('who', 295), ('can', 283), ('quite', 282), ('herself', 277), ('time', 277), ('some', 265), ('great', 265), ('too', 254), ('nothing', 254), ('has', 252), ('before', 250), ('most', 249), ('about', 249), ('fairfax', 241), ('dear', 240), ('always', 238), ('man', 235), ('thought', 226), ('soon', 224), ('may', 223), ('churchill', 223), ('see', 222), ('other', 221), ('shall', 219), ('again', 217), ('without', 214), ('frank', 208), ('out', 207), ('first', 206), ('father', 204), ('sure', 204), ('indeed', 202), ('made', 199), ('like', 197), ('body', 193), ('ever', 193), ('oh', 193), ('young', 191), ('up', 186), ('two', 179), ('though', 177), ('friend', 175), ('better', 172), ('then', 171), ('come', 171)]\n"
     ]
    }
   ],
   "source": [
    "# Solution 1.1\n",
    "from pathlib import Path\n",
    "\n",
    "def most_frequent_word(file_path, use_stop_list):\n",
    "    \"\"\"\n",
    "    Sort all words in a text-file by frequency.\n",
    "\n",
    "    arg1 = path to .txt file\n",
    "    arg2 = whether to exclude words that are in the stop_list, or not\n",
    "    \"\"\"\n",
    "    data_dict = {}\n",
    "    stop_list = ['the', 'a', 'an', 'is', 'was', 'were', 'be', 'had', 'have',\n",
    "                 'i', 'you', 'we', 'he', 'she', 'his', 'her', 'hers', 'this', 'that', 'there', 'they', \n",
    "                 'at', 'and', 'in', 'to', 'of', 'by', 'from', 'for', 'it', 'at', 'with', \n",
    "                 'but', 'now', 'not', 'so', 'as', 'very']\n",
    "\n",
    "    with open(file_path) as file:\n",
    "        data = file.read().lower()       # lees file als lowercase letters\n",
    "        data_list = data.split(' ')      # split str op spaties --> maakt er een list van\n",
    "\n",
    "        # ga voor elk woord tellen hoe vaak het voorkomt\n",
    "        for word in data_list:\n",
    "            try: \n",
    "                data_dict[word] += 1     # als de key nog niet bestaat, geeft dit een KeyError\n",
    "            except KeyError:\n",
    "                if use_stop_list:\n",
    "                    # excludeer woorden in stop_list\n",
    "                    if word not in stop_list:\n",
    "                        data_dict[word] = 1  # 1e keer dat het geteld wordt\n",
    "                else:\n",
    "                    # we hoeven niks te excluderen, tel gewoon alles\n",
    "                    data_dict[word] = 1  # 1e keer dat het geteld wordt\n",
    "        \n",
    "        # sorteer de dictionary op 'vaakst geteld'\n",
    "        sorted_list = sorted(data_dict.items(), key=lambda item: item[1], reverse=True)\n",
    "        print(f\"Most frequent word is '{sorted_list[0][0]}' with {sorted_list[0][1]} times \\n\")\n",
    "\n",
    "        return sorted_list[:100]  # print alleen de eerste 100 omdat je anders kunt scrollen tot je een ons weegt\n",
    "\n",
    "\n",
    "file_path = Path('data_14_pass_containers_for') / 'emma-tf.txt'  # definieer pad naar .txt file\n",
    "print(most_frequent_word(file_path, True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bc848c",
   "metadata": {},
   "source": [
    "## Exercise 2: Markov analysis (level of difficulty: 2)\n",
    "\n",
    "This exercise is inspired by / taken from the 'Think Python' (Allen B. Downey). \n",
    "\n",
    "In a Markov process, transition to the next state is only determined by the current state. Such Markov processes can be used to model e.g. text. In this exercises, you will implement such a text processing modeling system, and use it to generate text.\n",
    "\n",
    "In text processing, the 'current state' can be represented by the previous N words, and the predicted new state is the next word. We can build such a model by reading a text, and then for every set of N words, store each next word (or better: all possible next words, including their frequency). \n",
    "\n",
    "The first exercise is to create a function that builds such a model when reading a text file. The main task is to decide on the appropriate datastructures for building this model. In building the model, you must also take into account that the same model will be used to generate new text from this model.\n",
    "\n",
    "As an input file, the [emma.txt](https://github.com/AllenDowney/ThinkPython2/blob/master/code/emma.txt) has been prepared. All dashes, underscores and quotation marks have been stripped. What remains is text, with upper and lower case characters, and the following punctuation marks: ' , . : ; ! ? . For this exercise, you may consider these punctuation marks as words. For your convenience, the input text has been prepared such that these punctuation marks are separated by spaces before and after, such that all input words are space (space / newline / tab) separated. You do not need to change the case of the characters.\n",
    "\n",
    "In addition, there is a simpler text file, called test.txt in the data_14 folder. You may use that to e.g. print the result of the model building.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c9a4fe3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Dit': ['is'], 'is': ['een'], 'een': ['simpel'], 'simpel': ['test', 'test'], 'test': ['bestand', 'bestand'], 'bestand': ['voor', 'kan'], 'voor': ['het'], 'het': ['opbouwen', 'Markov', 'Markov', 'Markov', 'Markov'], 'opbouwen': ['van'], 'van': ['het'], 'Markov': ['model', 'model', 'model', 'model'], 'model': ['.', 'inderdaad', 'moet', 'geen'], '.': ['Met', 'Want'], 'Met': ['zo'], 'zo': [\"'\"], \"'\": ['n'], 'n': ['simpel'], 'kan': ['je', 'je'], 'je': ['kijken', 'met'], 'kijken': ['of'], 'of': ['het'], 'inderdaad': ['goed'], 'goed': ['gemaakt'], 'gemaakt': ['wordt'], 'wordt': ['.'], 'Want': ['het'], 'moet': ['natuurlijk'], 'natuurlijk': ['wel'], 'wel': ['kloppen'], 'kloppen': [','], ',': ['anders'], 'anders': ['kan'], 'met': ['het'], 'geen': ['zinnen'], 'zinnen': ['vormen'], 'vormen': ['.']}\n"
     ]
    }
   ],
   "source": [
    "# Solution 2.1\n",
    "from pathlib import Path\n",
    "\n",
    "def generate_model(file_path):\n",
    "    giant_list = {}\n",
    "\n",
    "    with open(Path('data_14_pass_containers_for') / file_path) as file:\n",
    "        text = file.read()\n",
    "        text_list = text.replace('\\n', ' ').split(' ')\n",
    "\n",
    "        for index, word in enumerate(text_list):\n",
    "            if index < len(text_list) - 1:\n",
    "                try:\n",
    "                    giant_list[word].append(text_list[index+1])\n",
    "                except:\n",
    "                    giant_list[word] = []\n",
    "                    giant_list[word].append(text_list[index+1])\n",
    "    return giant_list\n",
    "\n",
    "\n",
    "file_path = 'test.txt'\n",
    "print(generate_model(file_path))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9820a607",
   "metadata": {},
   "source": [
    "Write code to create random sentences. Think of a good way to initialize the first state in a nice and random manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5570bd41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "He thought so miserable state of from my uncle. Without knowing that I can it is, that it had been unnoticed, because she has been with a word might be kept it to be enduring by Miss Bates loved by their friendship for it not. But her taste of affection to value of hours; he had been a want to her life would be calling soon. Every body who had been so well class of Miss Taylor too; for though her convictions, with him. The good manners. Sighs and break up to expect that you were most excellent, and Miss Bates in a little more of her at the rest; and tried to prevent the two ladies of the distinguished honour of the wonder. Mr. John had often, when a distinct parties;--with so very delightful woman!--A style was equally ill-bestowed. It is nothing ungallant, nothing more useful now.”  “The joke,” he must not to a caution.--I wish to spend the first attempt on good wife. Remember.”    Redistribution is not play whenever it would be _broke_ to me keep the hall. Harriet felt immediately afterwards--gone in Mr. Dixon must not unlikely that she and faith engaged, and her "
     ]
    }
   ],
   "source": [
    "# Solution 2.2\n",
    "import random\n",
    "from time import sleep\n",
    "\n",
    "def text_processing(file_path, num_words):\n",
    "    # definieer mogelijke woorden om de output mee te beginnen...\n",
    "    start_list = ['This', 'He', 'She', 'They', 'You', 'I', 'So', 'But', 'However', 'It', 'Emma', 'Mr.', 'Mrs.' \n",
    "                  if file_path == 'emma.txt' else 'Dit', 'Met', 'Want']\n",
    "    # ... en kies random eentje\n",
    "    start_word = random.choice(start_list) \n",
    "\n",
    "    # 'train' het model op de .txt file\n",
    "    model = generate_model(file_path)\n",
    "\n",
    "    # print vast het 1e woord\n",
    "    output = start_word\n",
    "    print(output, end=' ')\n",
    "\n",
    "    for i in range(num_words):\n",
    "        # uit de array van mogelijke woorden na het vorige woord, dus output.split(' ')[-1] ...\n",
    "        # ... kies een random woord\n",
    "        new_word = random.choice(model[output.split(' ')[-1]])\n",
    "\n",
    "        # voeg toe aan huidige output, en print op dezelfde regel\n",
    "        output = output + \" \" + new_word\n",
    "        print(new_word, end=' ')\n",
    "\n",
    "        # random delay, net als ChatGPT, omdat het kan\n",
    "        sleep(random.choice([0, 0.02, 0.05, 0.07]))  # in seconden\n",
    "\n",
    "\n",
    "file_path = 'emma.txt'  # input file\n",
    "num_words = 200         # hoeveel woorden het moet genereren\n",
    "text_processing(file_path, num_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983d8c7c",
   "metadata": {},
   "source": [
    "# Exercise 2: Markov model for words (level of difficulty: 2)\n",
    "\n",
    "The same model building approach can be applied to modeling words instead of sentences. Do the same for words. In reading words, only include words of at least key_len + 1 char, and convert all words to lower case. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f864b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 3.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc22d2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution 3.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d4650dd",
   "metadata": {
    "tags": []
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a70d306b",
   "metadata": {},
   "source": [
    "Given the models build, you may want to get more information. E.g. which state has the highest number of possible next words. And how many different words are in that list. And how many states do have exactly one (deterministic) subsequent word. And what is the histogram of the number of words per state (i.e. visualize in a graph the number of states with exactly one word in the list, and with two words in the list, etc). And how do these histograms vary, e.g. as a function of key_len (what do you expect), of text (or author), or of language. It is left as a further exercise (without solution) to program solutions that may answer such questions. "
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_markers": "\"\"\""
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "af0757bbd091a27d1b459aad72cb64fd0f2b1eac8726eb2184f3794de23b07aa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
